{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "from modelVae_linear import Linear_Model\n",
    "from modelVae_convnet import Conv_Model\n",
    "from base_model import Base_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./weights/baseline_check2.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(PATH, map_location='cpu')\n",
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(checkpoint['model_state_dict'], \"./baseline_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading mix\n",
      "End loading mix\n"
     ]
    }
   ],
   "source": [
    "#parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "#parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "#                    help='input batch size for training (default: 128)')\n",
    "#parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "#                    help='number of epochs to train (default: 10)')\n",
    "#parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                    help='enables CUDA training')\n",
    "#parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                    help='random seed (default: 1)')\n",
    "#parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                    help='how many batches to wait before logging training status')\n",
    "#args = parser.parse_args()\n",
    "#args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "#torch.manual_seed(args.seed)\n",
    "\n",
    "#device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {}\n",
    "\n",
    "args_batch_size = 4\n",
    "semi = SemiSupervised(batch_size=args_batch_size)\n",
    "train_loader = semi.load_train_data_mix(transform=transforms.ToTensor())\n",
    "#test_loader = semi.load_val_data(transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modelVae_linear.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile modelVae_linear.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, x_dim=3*96*96, y_dim=3*96*96, encoder_dim=None, decoder_dim=None, latent_dim=20):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder_dim = []\n",
    "        self.decoder_dim = []\n",
    "\n",
    "        # Architecture\n",
    "        # TODO\n",
    "        # encoder_dim is a list of dimension\n",
    "        if encoder_dim is None:\n",
    "            self.encoder_dim = [x_dim] + [400, 20]\n",
    "        else:\n",
    "            self.encoder_dim = [x_dim] + encoder_dim\n",
    "        \n",
    "        if decoder_dim is None:\n",
    "            self.decoder_dim = [latent_dim] + [20, 400]\n",
    "        else:\n",
    "            self.decoder_dim = [latent_dim] + decoder_dim\n",
    "            \n",
    "        #encoder\n",
    "        #nets_en = [*self.encoder_dim]\n",
    "        linear_layers_en = [nn.Linear(self.encoder_dim[i-1], self.encoder_dim[i]) for i in range(1, len(self.encoder_dim))]\n",
    "        self.encoder_hidden = nn.ModuleList(linear_layers_en)\n",
    "        \n",
    "        # reparametrization for latent var\n",
    "        in_features = self.encoder_dim[-1]\n",
    "        out_features = latent_dim\n",
    "        \n",
    "        self.mu = nn.Linear(in_features, out_features)\n",
    "        self.log_var = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        #decode\n",
    "        #nets_de = [self.encoder_dim[-1], *self.decoder_dim]\n",
    "        linear_layers_de = [nn.Linear(self.decoder_dim[i-1], self.decoder_dim[i]) for i in range(1, len(self.decoder_dim))]\n",
    "        \n",
    "        self.decoder_hidden = nn.ModuleList(linear_layers_de)\n",
    "        self.reconstruction = nn.Linear(self.decoder_dim[-1], y_dim)\n",
    "\n",
    "        # Load pre-trained model\n",
    "        # self.load_weights('weights.pth')\n",
    "\n",
    "    def load_weights(self, pretrained_model_path, cuda=True):\n",
    "        # Load pretrained model\n",
    "        pretrained_model = torch.load(f=pretrained_model_path, map_location=\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        # Load pre-trained weights in current model\n",
    "        with torch.no_grad():\n",
    "            self.load_state_dict(pretrained_model, strict=True)\n",
    "\n",
    "        # Debug loading\n",
    "        print('Parameters found in pretrained model:')\n",
    "        pretrained_layers = pretrained_model.keys()\n",
    "        for l in pretrained_layers:\n",
    "            print('\\t' + l)\n",
    "        print('')\n",
    "\n",
    "        for name, module in self.state_dict().items():\n",
    "            if name in pretrained_layers:\n",
    "                assert torch.equal(pretrained_model[name].cpu(), module.cpu())\n",
    "                print('{} have been loaded correctly in current model.'.format(name))\n",
    "            else:\n",
    "                raise ValueError(\"state_dict() keys do not match\")\n",
    "    \n",
    "    def reparametrize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        for layer in self.encoder_hidden:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        # reparametrization\n",
    "        mu = self.mu(x)\n",
    "        log_var = F.softplus(self.log_var(x))\n",
    "        \n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        \n",
    "        for layer in self.decoder_hidden:\n",
    "            z = F.relu(layer(z))\n",
    "            \n",
    "        z = self.reconstruction(z)\n",
    "        return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modelVae_convnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modelVae_convnet.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "\n",
    "class Conv_Model(nn.Module):\n",
    "    def __init__(self, y_dim=3*96*96, decoder_dim=None, latent_dim=20):\n",
    "        super(Model, self).__init__()\n",
    "        self.decoder_dim = []\n",
    "\n",
    "        # Architecture\n",
    "        # TODO\n",
    "        # encoder_dim is a list of dimension\n",
    "        \n",
    "        if decoder_dim is None:\n",
    "            self.decoder_dim = [latent_dim] + [200, 4000]\n",
    "        else:\n",
    "            self.decoder_dim = [latent_dim] + decoder_dim\n",
    "            \n",
    "        #encoder\n",
    "        #nets_en = [*self.encoder_dim]\n",
    "        # input parameters: [[c_in, c_out, kernel_size, stride]]\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1), # 16* 92 * 92\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 16 * 46 * 46\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1), # 32 * 42 * 42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3, padding = 0)) # 32 * 14 * 14\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.fc1 = nn.Linear(14 * 14 * 32, latent_dim)\n",
    "        \n",
    "        self.encoder_hidden = nn.ModuleList([self.layer1, self.layer2, self.drop_out, self.fc1])\n",
    "        \n",
    "        # reparametrization for latent var\n",
    "        in_features = 14 * 14 * 32\n",
    "        out_features = latent_dim\n",
    "        \n",
    "        self.mu = nn.Linear(in_features, out_features)\n",
    "        self.log_var = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        #decode\n",
    "        #nets_de = [self.encoder_dim[-1], *self.decoder_dim]\n",
    "        linear_layers_de = [nn.Linear(self.decoder_dim[i-1], self.decoder_dim[i]) for i in range(1, len(self.decoder_dim))]\n",
    "        \n",
    "        self.decoder_hidden = nn.ModuleList(linear_layers_de)\n",
    "        self.reconstruction = nn.Linear(self.decoder_dim[-1], y_dim)\n",
    "\n",
    "        # Load pre-trained model\n",
    "        # self.load_weights('weights.pth')\n",
    "\n",
    "    def load_weights(self, pretrained_model_path, cuda=True):\n",
    "        # Load pretrained model\n",
    "        pretrained_model = torch.load(f=pretrained_model_path, map_location=\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        # Load pre-trained weights in current model\n",
    "        with torch.no_grad():\n",
    "            self.load_state_dict(pretrained_model, strict=True)\n",
    "\n",
    "        # Debug loading\n",
    "        print('Parameters found in pretrained model:')\n",
    "        pretrained_layers = pretrained_model.keys()\n",
    "        for l in pretrained_layers:\n",
    "            print('\\t' + l)\n",
    "        print('')\n",
    "\n",
    "        for name, module in self.state_dict().items():\n",
    "            if name in pretrained_layers:\n",
    "                assert torch.equal(pretrained_model[name].cpu(), module.cpu())\n",
    "                print('{} have been loaded correctly in current model.'.format(name))\n",
    "            else:\n",
    "                raise ValueError(\"state_dict() keys do not match\")\n",
    "    \n",
    "    def reparametrize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        for layer in self.encoder_hidden:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # reparametrization\n",
    "        mu = self.mu(x)\n",
    "        log_var = F.softplus(self.log_var(x))\n",
    "        \n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        \n",
    "        for layer in self.decoder_hidden:\n",
    "            z = F.relu(layer(z))\n",
    "            \n",
    "        z = self.reconstruction(z)\n",
    "        return torch.sigmoid(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile infer_model.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "from modelVae_linear import Linear_Model\n",
    "\n",
    "class Infer_model(nn.Module):\n",
    "    def __init__(self, ftm_path, model_path):\n",
    "        self.ft = Linear_Model()\n",
    "        self.ft.eval()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=5, stride=1), # 16* 92 * 92\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 16 * 46 * 46\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1), # 32 * 42 * 42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3, padding = 0)) # 32 * 14 * 14\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.fc1 = nn.Linear(14 * 14 * 32, 1000)\n",
    "        \n",
    "        self.classification = nn.ModuleList([self.layer1, self.layer2, self.drop_out, self.fc1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        add_feature = self.ft(x)\n",
    "        input_size = [*x.size()]\n",
    "        dim = np.prod(input_size)\n",
    "        \n",
    "        # resize the input data\n",
    "        # input data size is batch_size * 6 * 96 * 96\n",
    "        input_size[1] = 2*input_size[1]\n",
    "        new_data = torch.cat([x.view(-1, dim), add_feature.view(-1, dim)]).view(*input_size)\n",
    "        x = new_data\n",
    "        \n",
    "        for layer in self.classification:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing base_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile base_model.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "\n",
    "class Base_Model(nn.Module):\n",
    "    def __init__(self, n_class = 1000):\n",
    "        super(Base_Model, self).__init__()\n",
    "        \n",
    "\n",
    "        # Architecture\n",
    "        # TODO\n",
    "        # encoder_dim is a list of dimension\n",
    "    \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1), # 16* 92 * 92\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 16 * 46 * 46\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1), # 32 * 42 * 42\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3, padding = 0)) # 32 * 14 * 14\n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.fc1 = nn.Linear(14 * 14 * 32, 3*n_class)\n",
    "        self.fc2 = nn.Linear(3*n_class, n_class)\n",
    "        \n",
    "        self.encoder_hidden = nn.ModuleList([self.layer1, self.layer2, self.drop_out])\n",
    "\n",
    "        # Load pre-trained model\n",
    "        # self.load_weights('weights.pth')\n",
    "\n",
    "    def load_weights(self, pretrained_model_path, cuda=True):\n",
    "        # Load pretrained model\n",
    "        pretrained_model = torch.load(f=pretrained_model_path, map_location=\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        # Load pre-trained weights in current model\n",
    "        with torch.no_grad():\n",
    "            self.load_state_dict(pretrained_model, strict=True)\n",
    "\n",
    "        # Debug loading\n",
    "        print('Parameters found in pretrained model:')\n",
    "        pretrained_layers = pretrained_model.keys()\n",
    "        for l in pretrained_layers:\n",
    "            print('\\t' + l)\n",
    "        print('')\n",
    "\n",
    "        for name, module in self.state_dict().items():\n",
    "            if name in pretrained_layers:\n",
    "                assert torch.equal(pretrained_model[name].cpu(), module.cpu())\n",
    "                print('{} have been loaded correctly in current model.'.format(name))\n",
    "            else:\n",
    "                raise ValueError(\"state_dict() keys do not match\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        for layer in self.encoder_hidden:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        # reparametrization\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile train.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from SemiSupervised import SemiSupervised\n",
    "from modelVae_linear import Linear_Model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "semi = SemiSupervised(batch_size=args.batch_size)\n",
    "train_loader = semi.load_train_data_mix(transform=transforms.ToTensor())\n",
    "test_loader = semi.load_val_data(transform=transforms.ToTensor())\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3*96*96), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epoch = 10):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_mode_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = -1.0\n",
    "    for epoch in range(num_epoch):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epoch-1))\n",
    "        print('-'*10)\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            run_loss = 0.0\n",
    "            #run_correct = 0\n",
    "            proc = 0\n",
    "            \n",
    "            for inputs, _ in dataloaders[phase]:\n",
    "                proc += b_size\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                #labels = labels.to(device)\n",
    "                \n",
    "                #zeros para\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #forward\n",
    "                #track history if and only if in training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    recon_x, x, mu, logvar = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    #print(outputs.size())\n",
    "                    #print(preds)\n",
    "                    loss = criterion(recon_x, x, mu, logvar)\n",
    "                    \n",
    "                    # backwarda and optimize only in training\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                if proc % 640 == 0:\n",
    "                    print('processed photos are {}'.format(proc))\n",
    "                # statistics\n",
    "                run_loss += loss.item()*inputs.size(0)\n",
    "                #run_correct += torch.sum(labels.data == preds)\n",
    "                \n",
    "                \n",
    "            epoch_loss = run_loss/dataset_sizes[phase]\n",
    "            #epoch_acc = run_correct/dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if best_loss < 0:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepycopy(mode.state_dict())\n",
    "                \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepycopy(mode.state_dict())\n",
    "                \n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "    #print('Best Acc: {:.4f}'.format(best_acc))\n",
    "        \n",
    "    model.load(best_model_wts)\n",
    "        \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    save_path = './vae_linear.pt'\n",
    "    model = Linear_Model()\n",
    "    \n",
    "    criterion = loss_function\n",
    "\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamma = 0.1)\n",
    "\n",
    "    model_best = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epoch = 10)\n",
    "    torch.save(model_best.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 96, 96])\n",
      "torch.Size([589824])\n",
      "[64, 1, 96, 96]\n",
      "589824\n",
      "589824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2, 96, 96])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(64, 1, 96, 96)\n",
    "b = torch.zeros(64*1*96*96)\n",
    "input_size = [*a.size()]\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(input_size)\n",
    "dim = np.prod(input_size)\n",
    "print(dim)\n",
    "print(64*1*96*96)\n",
    "input_size[1] = 2*input_size[1]\n",
    "new_data = torch.cat([a.view(-1,dim), b.view(-1, dim)]).view(*input_size)\n",
    "new_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([589824])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1, dim).size()\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    if batch_idx == 0:\n",
    "        print(data.size())\n",
    "        #print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = torch.randn(64, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([1,2,3])\n",
    "b = torch.FloatTensor([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  5.,  6.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6487,  2.7183,  4.4817])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(0.5*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8990,  0.8012, -0.0942])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-c7a522a0593b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-c7a522a0593b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    help torch.randn_like()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "help torch.randn_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = torch.cat([a[:8], b[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = torch.ones(64*96*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = d.view(*size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 96, 96])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()\n",
    "a[:, 0:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 96, 96])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.cat([a[1,:, :, :], e[1,:, :, :]])\n",
    "f.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "binary_cross_entropy() got an unexpected keyword argument 'reduction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c6510d40e2ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27648\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27648\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: binary_cross_entropy() got an unexpected keyword argument 'reduction'"
     ]
    }
   ],
   "source": [
    "recon_x = torch.ones(128, 27648)\n",
    "x = torch.ones(128, 27648)\n",
    "F.binary_cross_entropy(recon_x, x.view(-1, 3*96*96), reduction = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'where' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e4ff6325a5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'where' is not defined"
     ]
    }
   ],
   "source": [
    "where(x < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (Tensor, requires_grad=bool), but expected one of:\n * (torch.device device)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (object data, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e4e5e1adee28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27648\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (Tensor, requires_grad=bool), but expected one of:\n * (torch.device device)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (object data, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n"
     ]
    }
   ],
   "source": [
    "mu = torch.ones(128, 27648)\n",
    "torch.Tensor(torch.randn(mu.size()), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
